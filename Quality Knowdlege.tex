\documentclass[11pt]{article}

    \usepackage[breakable]{tcolorbox}
    \usepackage{parskip} % Stop auto-indenting (to mimic markdown behaviour)
    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % Maintain compatibility with old templates. Remove in nbconvert 6.0
    \let\Oldincludegraphics\includegraphics
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionFormat{nocaption}{}
    \captionsetup{format=nocaption,aboveskip=0pt,belowskip=0pt}

    \usepackage{float}
    \floatplacement{figure}{H} % forces figures to be placed at the correct location
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro

    \usepackage{iftex}
    \ifPDFTeX
        \usepackage[T1]{fontenc}
        \IfFileExists{alphabeta.sty}{
              \usepackage{alphabeta}
          }{
              \usepackage[mathletters]{ucs}
              \usepackage[utf8x]{inputenc}
          }
    \else
        \usepackage{fontspec}
        \usepackage{unicode-math}
    \fi

    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics
                         % to support a larger range
    \makeatletter % fix for old versions of grffile with XeLaTeX
    \@ifpackagelater{grffile}{2019/11/01}
    {
      % Do nothing on new versions
    }
    {
      \def\Gread@@xetex#1{%
        \IfFileExists{"\Gin@base".bb}%
        {\Gread@eps{\Gin@base.bb}}%
        {\Gread@@xetex@aux#1}%
      }
    }
    \makeatother
    \usepackage[Export]{adjustbox} % Used to constrain images to a maximum size
    \adjustboxset{max size={0.9\linewidth}{0.9\paperheight}}

    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    % The default LaTeX title has an obnoxious amount of whitespace. By default,
    % titling removes some of it. It also provides customization options.
    \usepackage{titling}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage{array}     % table support for pandoc >= 2.11.3
    \usepackage{calc}      % table minipage width calculation for pandoc >= 2.11.1
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{mathrsfs}
    \usepackage{amsmath}
    \usepackage{nccmath}
    \usepackage{pgfplots}
    \usepackage{subcaption}
    \usepackage{hyperref}

    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % common color for the border for error outputs.
    \definecolor{outerrorbackground}{HTML}{FFDFDF}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}

    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}


    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}
    % Document parameters
    % Document title
    \title{Statistical Process Control}
    \author{Jesus Cardenaz}
    
    
    
    
    
% Pygments definitions
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\@namedef{PY@tok@w}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\@namedef{PY@tok@c}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cp}{\def\PY@tc##1{\textcolor[rgb]{0.61,0.40,0.00}{##1}}}
\@namedef{PY@tok@k}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kt}{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\@namedef{PY@tok@o}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ow}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@nb}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nf}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@ne}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.80,0.25,0.22}{##1}}}
\@namedef{PY@tok@nv}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@no}{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\@namedef{PY@tok@nl}{\def\PY@tc##1{\textcolor[rgb]{0.46,0.46,0.00}{##1}}}
\@namedef{PY@tok@ni}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PY@tok@na}{\def\PY@tc##1{\textcolor[rgb]{0.41,0.47,0.13}{##1}}}
\@namedef{PY@tok@nt}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nd}{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@s}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sd}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@si}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PY@tok@se}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.36,0.12}{##1}}}
\@namedef{PY@tok@sr}{\def\PY@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PY@tok@ss}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sx}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@m}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@gh}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@gu}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\@namedef{PY@tok@gd}{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\@namedef{PY@tok@gi}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.52,0.00}{##1}}}
\@namedef{PY@tok@gr}{\def\PY@tc##1{\textcolor[rgb]{0.89,0.00,0.00}{##1}}}
\@namedef{PY@tok@ge}{\let\PY@it=\textit}
\@namedef{PY@tok@gs}{\let\PY@bf=\textbf}
\@namedef{PY@tok@gp}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@go}{\def\PY@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PY@tok@gt}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\@namedef{PY@tok@err}{\def\PY@bc##1{{\setlength{\fboxsep}{\string -\fboxrule}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}}
\@namedef{PY@tok@kc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kd}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kr}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@bp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@fm}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@vc}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vg}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vi}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vm}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sa}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sb}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sc}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@dl}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s2}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sh}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s1}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@mb}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mf}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mh}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mi}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@il}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mo}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ch}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cm}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cpf}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@c1}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cs}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % For linebreaks inside Verbatim environment from package fancyvrb.
    \makeatletter
        \newbox\Wrappedcontinuationbox
        \newbox\Wrappedvisiblespacebox
        \newcommand*\Wrappedvisiblespace {\textcolor{red}{\textvisiblespace}}
        \newcommand*\Wrappedcontinuationsymbol {\textcolor{red}{\llap{\tiny$\m@th\hookrightarrow$}}}
        \newcommand*\Wrappedcontinuationindent {3ex }
        \newcommand*\Wrappedafterbreak {\kern\Wrappedcontinuationindent\copy\Wrappedcontinuationbox}
        % Take advantage of the already applied Pygments mark-up to insert
        % potential linebreaks for TeX processing.
        %        {, <, #, %, $, ' and ": go to next line.
        %        _, }, ^, &, >, - and ~: stay at end of broken line.
        % Use of \textquotesingle for straight quote.
        \newcommand*\Wrappedbreaksatspecials {%
            \def\PYGZus{\discretionary{\char`\_}{\Wrappedafterbreak}{\char`\_}}%
            \def\PYGZob{\discretionary{}{\Wrappedafterbreak\char`\{}{\char`\{}}%
            \def\PYGZcb{\discretionary{\char`\}}{\Wrappedafterbreak}{\char`\}}}%
            \def\PYGZca{\discretionary{\char`\^}{\Wrappedafterbreak}{\char`\^}}%
            \def\PYGZam{\discretionary{\char`\&}{\Wrappedafterbreak}{\char`\&}}%
            \def\PYGZlt{\discretionary{}{\Wrappedafterbreak\char`\<}{\char`\<}}%
            \def\PYGZgt{\discretionary{\char`\>}{\Wrappedafterbreak}{\char`\>}}%
            \def\PYGZsh{\discretionary{}{\Wrappedafterbreak\char`\#}{\char`\#}}%
            \def\PYGZpc{\discretionary{}{\Wrappedafterbreak\char`\%}{\char`\%}}%
            \def\PYGZdl{\discretionary{}{\Wrappedafterbreak\char`\$}{\char`\$}}%
            \def\PYGZhy{\discretionary{\char`\-}{\Wrappedafterbreak}{\char`\-}}%
            \def\PYGZsq{\discretionary{}{\Wrappedafterbreak\textquotesingle}{\textquotesingle}}%
            \def\PYGZdq{\discretionary{}{\Wrappedafterbreak\char`\"}{\char`\"}}%
            \def\PYGZti{\discretionary{\char`\~}{\Wrappedafterbreak}{\char`\~}}%
        }
        % Some characters . , ; ? ! / are not pygmentized.
        % This macro makes them "active" and they will insert potential linebreaks
        \newcommand*\Wrappedbreaksatpunct {%
            \lccode`\~`\.\lowercase{\def~}{\discretionary{\hbox{\char`\.}}{\Wrappedafterbreak}{\hbox{\char`\.}}}%
            \lccode`\~`\,\lowercase{\def~}{\discretionary{\hbox{\char`\,}}{\Wrappedafterbreak}{\hbox{\char`\,}}}%
            \lccode`\~`\;\lowercase{\def~}{\discretionary{\hbox{\char`\;}}{\Wrappedafterbreak}{\hbox{\char`\;}}}%
            \lccode`\~`\:\lowercase{\def~}{\discretionary{\hbox{\char`\:}}{\Wrappedafterbreak}{\hbox{\char`\:}}}%
            \lccode`\~`\?\lowercase{\def~}{\discretionary{\hbox{\char`\?}}{\Wrappedafterbreak}{\hbox{\char`\?}}}%
            \lccode`\~`\!\lowercase{\def~}{\discretionary{\hbox{\char`\!}}{\Wrappedafterbreak}{\hbox{\char`\!}}}%
            \lccode`\~`\/\lowercase{\def~}{\discretionary{\hbox{\char`\/}}{\Wrappedafterbreak}{\hbox{\char`\/}}}%
            \catcode`\.\active
            \catcode`\,\active
            \catcode`\;\active
            \catcode`\:\active
            \catcode`\?\active
            \catcode`\!\active
            \catcode`\/\active
            \lccode`\~`\~
        }
    \makeatother

    \let\OriginalVerbatim=\Verbatim
    \makeatletter
    \renewcommand{\Verbatim}[1][1]{%
        %\parskip\z@skip
        \sbox\Wrappedcontinuationbox {\Wrappedcontinuationsymbol}%
        \sbox\Wrappedvisiblespacebox {\FV@SetupFont\Wrappedvisiblespace}%
        \def\FancyVerbFormatLine ##1{\hsize\linewidth
            \vtop{\raggedright\hyphenpenalty\z@\exhyphenpenalty\z@
                \doublehyphendemerits\z@\finalhyphendemerits\z@
                \strut ##1\strut}%
        }%
        % If the linebreak is at a space, the latter will be displayed as visible
        % space at end of first line, and a continuation symbol starts next line.
        % Stretch/shrink are however usually zero for typewriter font.
        \def\FV@Space {%
            \nobreak\hskip\z@ plus\fontdimen3\font minus\fontdimen4\font
            \discretionary{\copy\Wrappedvisiblespacebox}{\Wrappedafterbreak}
            {\kern\fontdimen2\font}%
        }%

        % Allow breaks at special characters using \PYG... macros.
        \Wrappedbreaksatspecials
        % Breaks at punctuation characters . , ; ? ! and / need catcode=\active
        \OriginalVerbatim[#1,codes*=\Wrappedbreaksatpunct]%
    }
    \makeatother

    % Exact colors from NB
    \definecolor{incolor}{HTML}{303F9F}
    \definecolor{outcolor}{HTML}{D84315}
    \definecolor{cellborder}{HTML}{CFCFCF}
    \definecolor{cellbackground}{HTML}{F7F7F7}

    % prompt
    \makeatletter
    \newcommand{\boxspacing}{\kern\kvtcb@left@rule\kern\kvtcb@boxsep}
    \makeatother
    \newcommand{\prompt}[4]{
        {\ttfamily\llap{{\color{#2}[#3]:\hspace{3pt}#4}}\vspace{-\baselineskip}}
    }
    

    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

\begin{document}
    
    \maketitle
    
    

    
    \hypertarget{statistics-refresher}{%
\section{Statistics Refresher}\label{statistics-refresher}}

    \hypertarget{basic-statistics-an-shit}{%
\subsection{Basic Statistics and shit}\label{basic-statistics-an-shit}}

    \hypertarget{mean-variance-and-standard-deviation.}{%
\subsubsection{Mean, Variance and Standard
Deviation.}\label{mean-variance-and-standard-deviation.}}

\hypertarget{mean}{%
\paragraph{Mean}\label{mean}}

The mean of a dataset is a way of capturing the `center of mass' of a
distribution. It is a way of calculating the expected value \(\mu\) of a
random variable \(X\).

The expected value \(\mu\) is a measure of the central tendency of a
random variable, and it represents the \emph{average} value that we
would expect to observe if we repeated the random experiment many times.
It is defined by the following equation:

\begin{gather}
  \mu = E[X] = \sum_{x} P(X = x) \cdot x
\end{gather}

Where \(x\) Represents values of the random variable \(X\).

You go through all possible outcomes and you multiply the probability of
that outcome times the value of the variable.



\hypertarget{variance}{%
\paragraph{Variance}\label{variance}}

The variance is a measure of how spread out the values of the random
variable are from the expected value or the mean.

\begin{gather}
  \sigma^2 = Var(X) = E[(X- \mu)^2] = \sum_x P(X=x) \cdot (x - \mu)^2 
\end{gather}
  


The idea is to look at the difference between each possible value and
the mean, square that difference and ask for its expected value. This
way, if the expected value is above or bellow the mean, it's still a
positive number and the greater the difference, the bigger the number.

\(E[(X - \mu)^2]\): This is the expectation of the squared difference
between the random variable X and its expected value or the mean
(\(\mu\)). We square the difference to ensure that the value is always
positive.

\(\sum_x P(X=x) \cdot (x - \mu)^2\): This is the expanded form of the
expected value. It involves summing up the products of the probabilities
of each possible value of X and the squared difference between that
value and the mean.


\hypertarget{standard-deviation}{%
\paragraph{Standard Deviation}\label{standard-deviation}}

The problem of the variance is that it's difficult to interpret this
value as a `distance from the mean' since is a squared value. So a more
common way of interpreting spread is the Standard Deviation The Standard
Deviation is another way of measure spread, it is the square root of the
variance. It makes it easier to interpret the spread of a dataset.

It measures how much the data values deviate from the mean or the
expected value of the data set. A high standard deviation indicates that
the data values are spread out over a larger range, while a low standard
deviation indicates that the data values are clustered around the mean.

\hypertarget{Parameters vs Statistics}{%
\paragraph*{Parameters vs statistics}\label{parameters-vs-statistics}}
Parameters are numbers that describe the properties of
entire populations. Statistics are numbers that describe the properties
of samples. For example, the average income for the United States is a
population parameter. Conversely, the average income for a sample drawn
from the U.S. is a sample statistic. Both values represent the mean
income, but one is a parameter vs a statistic.

\begin{table}[h]
  \centering
  \begin{tabular}{|c|c|c|}
  \hline
  \textbf{Summary Value} & \textbf{Parameter} & \textbf{Statistic} \\ \hline
  Mean & $\mu$ or Mu & $\bar{x}$ or x-bar \\ \hline
  Standard deviation & $\sigma$ or Sigma & $s$ \\ \hline
  Correlation & $\rho$ or rho & $r$ \\ \hline
  Proportion & $P$ & $\hat{p}$ or p-hat \\ \hline
  \end{tabular}
  \caption{Summary Statistics}
  \label{tab:summary}
\end{table}
  

\hypertarget{probability-distributions}{%
\subsubsection{Probability Distributions}\label{probability-distributions}}

A probability distribution is the \textbf{mathematical function} that
gives the probabilities of occurrence of different possible outcomes for
a random variable. It describes the distribution of the values that a
random variable can take and their probabilities of occurrence.

There are two main ways to represent the distribution of a set of data
Frequency Distributions and Density Curves.

\hypertarget{frequency-distributions}{%
\paragraph{Frequency Distributions}\label{frequency-distributions}}

A frequency distribution describes the number of observations for
each possible value of a variable. The
frequency of a value is the number of times it occurs in a dataset. A
frequency distribution is the pattern of frequencies of a variable. It's
the number of times each possible value of a variable occurs in a
dataset.

Mainly there are two kinds of frequency distributions: Regular
Frequency Distribution which tells us the number of values within certain 
given
intervals. And Relative Frequency Distribution, which tells us the proportion of
values within any given interval.

\hypertarget{density-curves}{%
\paragraph{Density Curves}\label{density-curves}}

A density curve is a \textbf{graph} that shows the probability of
outcomes of a variable. It helps us visualize the overall shape of the
distribution.


Properties of density curves: - A density curve mus line on or above the
horizontal axis. - The total area of the curve must equal to 1.

\hypertarget{normal-distribution}{%
\subsubsection{Normal Distribution}\label{normal-distribution}}

A normal distribution is a special type of density curve that is
bell-shaped. In statistics, a \textbf{normal distribution} or
\textbf{Gaussian distribution} is a type of continuous probability
distribution for a real-valued random variable. The general form of its
probability density function is:

\begin{gather}
  f(x) = \frac{1}{\sigma \sqrt{2\pi}}
  \ e^{-\frac{1}{2}(\frac{x - \mu }{\sigma})^2}
\end{gather}


\paragraph{Building up the formula}
To understand the formula, we're going to peel it off to the most basic
form, and then build it up it step by step.

The function \(e^x\) or anything to the \(x\) describes exponential
growth. 
Then, if we graph \(e^{-x}\) it'll describe exponential decay

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
    \begin{tikzpicture}
      \begin{axis}[          xlabel=$x$,          ylabel=$y$,          xmin=-2, xmax=2,          ymin=0, ymax=4,          axis lines=middle,          thick,          samples=100,          domain=-2:2,          ]
      \addplot [blue] {exp(x)};
      \addlegendentry{\(e^{x}\)}
      \end{axis}
    \end{tikzpicture}
    \caption{Plot 1}
    \label{fig:plot1}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.45\textwidth}
    \begin{tikzpicture}
      \begin{axis}[          xlabel=$x$,          ylabel=$y$,          xmin=-2, xmax=2,          ymin=0, ymax=4,          axis lines=middle,          thick,          samples=100,          domain=-2:2,          ]
      \addplot [red] {exp(-x)};
      \addlegendentry{\(e^{-x}\)}
      \end{axis}
    \end{tikzpicture}
    \caption{Plot 2}
    \label{fig:plot2}
  \end{subfigure}
  \caption{Two Plots Side by Side}
  \label{fig:plots}
\end{figure}

So if we want our graph to be symmetrical, we could do something like
\(e^{-|x|}\)
But that would make a sharp point at \(x=0\), so a better solution would be to
square the \(x\) value.

This already gave us the basic bell-shape we were looking for.

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
    \begin{tikzpicture}
      \begin{axis}[          xlabel=$x$,          ylabel=$y$,          xmin=-2, xmax=2,          ymin=0, ymax=4,          axis lines=middle,          thick,          samples=100,          domain=-2:2,          ]
      \addplot [blue] {exp(-abs(-x))};
      \addlegendentry{\(e^{-|x|}\)}
      \end{axis}
    \end{tikzpicture}
    \caption{Plot 1}
    \label{fig:plot1}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.45\textwidth}
    \begin{tikzpicture}
      \begin{axis}[          xlabel=$x$,          ylabel=$y$,          xmin=-2, xmax=2,          ymin=0, ymax=4,          axis lines=middle,          thick,          samples=100,          domain=-2:2,          ]
      \addplot [red] {exp((-x^2))};
      \addlegendentry{\(e^{-x^2}\)}
      \end{axis}
    \end{tikzpicture}
    \caption{Plot 2}
    \label{fig:plot2}
  \end{subfigure}
  \caption{Two Plots Side by Side}
  \label{fig:plots}
\end{figure}




Now for the interesting part, if we trow a constant in front of the \(x\), this allows us to stretch and
squish the graph horizontally. Allowing us describe narrow and wider
bell curves.  

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
    \begin{tikzpicture}
      \begin{axis}[          xlabel=$x$,          ylabel=$y$,          xmin=-2, xmax=2,          ymin=0, ymax=4,          axis lines=middle,          thick,          samples=100,          domain=-2:2,          ]
      \addplot [blue] {exp(-7*x^2)};
      \addlegendentry{\(e^{-7x^2}\)}
      \end{axis}
    \end{tikzpicture}
    \caption{Plot 1}
    \label{fig:plot1}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.45\textwidth}
    \begin{tikzpicture}
      \begin{axis}[          xlabel=$x$,          ylabel=$y$,          xmin=-2, xmax=2,          ymin=0, ymax=4,          axis lines=middle,          thick,          samples=100,          domain=-2:2,          ]
      \addplot [red] {exp(-.5*x^2)};
      \addlegendentry{\(e^{-0.5x^2}\)}
      \end{axis}
    \end{tikzpicture}
    \caption{Plot 2}
    \label{fig:plot2}
  \end{subfigure}
  \caption{Two Plots Side by Side}
  \label{fig:plots}
\end{figure}

So if we want to describe the function by a value \(\sigma\) we would do
something like: \(e^{-\frac{1}{2}(x/\sigma)^2}\) But before we can
define our function by the standard deviation, we want it to be a
probability density function (PDF).

So we want this function to be a probability density function (PDF),
that meaning that the total area under the curve of the function should
be equal to one, this can be accomplished
\href{https://youtu.be/cy8r7WSuT1I}{dividing by the square root of
pi}:

$$\frac{1}{\sqrt\pi} e^{-x^2}$$

But, we would like to describe our function by the standard deviation
\(\sigma\), so we also need to divide by that in order to the curve to
still have an area of 1.

$$ \frac{1}{\sigma \sqrt{2\pi}} \ e^{-\frac{1}{2} \left (\frac{x}{\sigma} \right)^2} $$

And this is already a valid normal distribution. Tweaking the value
\(\sigma\) resulting in narrower or wider curves, would still mean that
the area under the curve is one.

\hypertarget{standard-normal-distribution}{%
\paragraph{Standard Normal Distribution}\label{standard-normal-distribution}}
The case where the standard deviation is one, is called Standard
Normal Distribution and is given by:

\begin{gather}
  \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}x^2}
\end{gather}

Finally, if we subtract the mean \(\mu\) we can characterize all
possible normal distributions, and this gives us the final Standard Normal Distribution 
formula that we've seen.


\[\frac{1}{\sigma \sqrt{2\pi}} \ e^{-\frac{1}{2}(\frac{x - \mu }{\sigma})^2}\]

\hypertarget{properties-of-the-normal-distribution}{%
\paragraph{Properties}\label{properties-of-the-normal-distribution}}

The normal distribution is characterized by the following properties:
\begin{itemize}
\item The normal distribution is unimodal, which means that it has only one peak.
\item The normal curve is symmetric about its mean.
\item The parameters Î¼ and \sigma completely characterize the normal distribution.
\item \(X \sim N(\mu,\sigma)\)
\end{itemize}

The notation \(X \sim N(\mu,\sigma)\) represents a random variable \(X\) that follows
a normal distribution with mean \(\mu\) and standard deviation \(\sigma\).

\hypertarget{central-limit-theorem}{%
\subsubsection{TODO: Central Limit Theorem}\label{central-limit-theorem}}
The central limit theorem states that the distribution of a random
variable in a sample will begin to approach a normal distribution as the
sample size becomes larger, regardless of the true shape of the
distribution.

Consider
\(\frac{(X_1 + \ ... \ +X_N ) - N \cdot \mu}{\sigma \cdot \sqrt{N}}\)

\hypertarget{statistical-models}{%
\subsection{Statistical models}\label{statistical-models}}

A statistical model is a mathematical representation of observed data.
When data analysts apply various statistical models to the data they are
investigating, they are able to understand and interpret the information
more strategically. \#\#\# Classification Models Classification is a
process in which an algorithm is used to analyze an existing data set of
known points.

\hypertarget{regression-models}{%
\subsection{TODO: Regression Models}\label{regression-models}}

Regression models are used to examine relationships between variables.
Regression models are often used to determine which independent
variables hold the most influence over dependent variables.
\begin{itemize}
  \item Stepwise Regression.
  \item Ridge regression.
  \item Lassso regression. 
  \item Elastic net regression.
\end{itemize}

\hypertarget{statistical-hypothesis}{%
\subsection{Statistical Hypothesis}\label{statistical-hypothesis}}

A statement about the nature of a population. It is often stated in
terms of a population parameter. It is a formal claim about a state of
nature structure within the framework of a statistical model.

The word hypothesis means a working statement. In statistics, we are
interested in proving whether a working statement (the null hypothesis)
is true or false.

In SPC, null-hypothesis = process in control, alter-hypothesis = process
not in control.

\hypertarget{null-hypothesis-h_0}{%
\subsubsection{Null-Hypothesis \(H_0\)}\label{null-hypothesis-h_0}}

It states that the results are due to chance and are not significant in
terms of supporting the idea of being investigated. The null hypothesis
states that there is no relationship between the two variables being
studied.

\emph{``The null hypothesis is a typical statistical theory which
suggests that no statistical relationship and significance exists in a
set of given single observed variable, between two sets of observed data
and measured phenomena.''}

In SPC, the null hypothesis usually states that the process is under
control, and any observed variation is due to chance.

\hypertarget{alternative-hypothesis.-h_a}{%
\subsubsection{Alternative hypothesis. \(H_a\)}\label{alternative-hypothesis.-h_a}}

Represents a hypothesis of observations which are influenced by some
non-random cause. The alternative hypothesis states that the independent
variable did affect the dependent variable, and the results are
significant in terms of supporting the theory being investigated. (Not
due to chance.)

In SPC, proving that the alternative hypothesis is true means that the
process is not under statistical control, and it's affected by some
non-random variation (special variation).

\hypertarget{type_1_errors}{%
\subsubsection{Type I Errors}\label{type_1_errors}}

\hypertarget{type_2_errors}{%
\subsubsection{Type II Errors}\label{type_2_errors}}

\hypertarget{p-values}{%
\subsubsection{P Values}\label{p-values}}
The p-value in is the probability that the measured difference would occur 
due to random chance alone if the null hypothesis were true. 



\hypertarget{significance-levels}{%
\subsubsection{Significance Levels}\label{significance-levels}}


\hypertarget{directional-hypothesis}{%
\paragraph{TODO - Add Image: Directional Hypothesis}\label{directional-hypothesis}}

In the Directional Hypothesis, the null hypothesis is rejected if the
test score is too large or too small. Thus, the rejection region for
such a test consist of one part, which is on the right side for a
right-tailed test or the rejection region is on the left side from the
center in the case of a left-tailed test.

\hypertarget{non-directional-hypothesis}{%
\paragraph{TODO - Add Image: Non-Directional Hypothesis}\label{non-directional-hypothesis}}

In a non-directional hypothesis test, the null hypothesis is rejected if
the test score is either too small or too large. Thus, the rejection
region for such test consist of two parts, one on the left and one on
the right.

\hypertarget{why-not-accept-the-null-hypothesis}{%
\paragraph{Why not accept the Null-Hypothesis?}\label{why-not-accept-the-null-hypothesis}}

We assume that the null hypothesis is correct until we have enough
evidence to suggest otherwise.

After you perform a hypothesis test, there are only two possible
outcomes: - When your p-value is less than or equal to your significance
level, you \emph{reject the null hypothesis}. The data favors the
alternative hypothesis. \textbf{Your results are statistically
significant.}

\begin{itemize}
\tightlist
\item
  When your p-value is greater than your significance level, \emph{you
  fail to reject the null hypothesis}. \textbf{Your results are not
  significant}
\end{itemize}

\hypertarget{failure-to-reject-the-null}{%
\paragraph{Failure to Reject the Null}\label{failure-to-reject-the-null}}

A lack of evidence isn't proof that something doesn't exist. You just
haven't proven that it exists. It might exist, but your study missed it.
That's a huge difference, and it is the reason for the convoluted
wording.




\hypertarget{criminal-trials}{%
\paragraph{Criminal Trials}\label{criminal-trials}}

In a trial, we start with the assumption that the defendant is innocent
until proven guilty. The prosecutor must work hard to exceed an
evidentiary standard to obtain a guilty verdict. If the prosecutor does
not meet that burden, it doesn't prove the defendant is innocent.
Instead, there was insufficient evidence to conclude he is guilty.

Perhaps the prosecutor conducted a shoddy investigation and missed
clues, or the defendant successfully covered his tracks. Consequentially
the verdict in these cases is \emph{Not Guilty}. That judgment doesn't
say the defendant is proven innocent, just that there wasn't enough
evidence to move the jury from the default assumption of innocence.

The hypothesis test assesses the evidence in your sample. If your test
fails to detect an effect, that's not proof it doesn't exist. It just
means your sample contained an insufficient amount of evidence to
conclude that it exists. Like the prosecutor who missed clues, the
effect might exist in the overall population but not in your particular
sample. Consequently the test results \emph{fails to reject the null
hypothesis}, which is analogous to a `not guilty' verdict in a trial.
There was not enough evidence to move the hypothesis test from the
default position that the null is true.

Accepting the null hypothesis would indicate that you've proven that an
effect doesn't exist. You can't prove a negative. Failing to reject the
null indicates that our sample did not provide sufficient evidence to
conclude that the effect exists, that lack of evidence doesn't prove
that the effect does not exist.

\hypertarget{example}{%
\paragraph{Example}\label{example}}
Suppose we wanted to check whether a coin was fair and balanced. A null
hypothesis might say, that half flips will be of head and the other half
will be tails. Whereas alternative hypothesis might say that flips of
head and tail may be very different.
For example if we flipped the coin 50 times, in which 40 heads an 10
tails results.

\hypertarget{statistical-measures}{%
\subsection{Statistical Measures}\label{statistical-measures}}

\hypertarget{z-score}{%
\subsubsection{Z Score}\label{z-score}}

Z-score is a statistical measurement that describes a value's
relationship to the mean of a group of values. Z-score is measured in
terms of standard deviations from the mean. If a Z-score is 0, it
indicates that the data point's score is identical to the mean score.

The formula for the Z-score: 
\begin{gather}
  z=\frac{(x-\mu)}{\sigma}
\end{gather}


The higher or lower a z-score is, the further away from the mean the
point is.

It is also known as a standard score, because it allows comparison of
scores on different kinds of variables by standardizing the
distribution. A Standard Normal Distribution (SND) is a normally shaped
distribution with a mean of 0 and a standard deviation of 1.

\begin{itemize}
\item
  The Z-Score allows to calculate the probability of a score occurring
  within a standard normal distribution.
\item
  Enables comparing two scores that are from different samples (which
  may have different means and standard deviations).
\end{itemize}

In SPC, the z-score is used to determine whether a data point is an
outlier or falls within the expected range of values. A z-score can be
converted into a p-value using a standard normal distribution table or
calculator.

\hypertarget{critical-values.}{%
\subsubsection{Critical Values.}\label{critical-values.}}

In statistics, a critical value refer to specific values that are used
to determine whether to reject or fail to reject a null hypothesis in a
statistical test.

The critical value is the threshold value beyond which we reject the
null hypothesis. It is based on the significance level (alpha level) of
the test.

In SPC, the critical values are often used to define the control limits.

\hypertarget{significance-level-alpha}{%
\subsubsection{\texorpdfstring{Significance Level
\(\alpha\)}{Significance Level \textbackslash alpha}}\label{significance-level-alpha}}

The significance level defines how strong the sample evidence must be to
conclude an effect exists in the population.

The significance level, also known as alpha or \(\alpha\), is an
evidentiary standard that researchers set before the study.

\hypertarget{p-values}{%
\subsubsection{P Values}\label{p-values}}

The p-value is a number, calculated from a statistical test, that describes how
likely you are to have found a particular set of observations if the null hypothesis were true.


When you perform a statistical test, a p-value helps you determine the
significance of your results in relation to the null hypothesis. It is a
number describing how likely it is that your data would have occurred by
random chance. (Null hypothesis is true). The level of statistical
significance is often expressed as a p-value between 0 and 1. The
smaller the P-value the stronger the evidence that you should reject the
null hypothesis.

\begin{itemize}
\item
  A p-value less than 0.05 is statistically significant. It indicates
  strong evidence against the null hypothesis, as there is less that 5\%
  probability the null is correct. (Results are random).
\item
  A p-value higher than 0.05 is not statistically significant and
  indicates strong evidence for the null hypothesis.
\end{itemize}

The p-value calculates the likelihood of your test statistic. In other words, 
the p-value tells you how often you would expect to see a test statistic as extreme
or more extreme than the one calculated by yous statistical test if the null hypothesis of
that test was true. 

\paragraph{Calculation of the P-value} The calculation of the p-value depends
on the statistical test you are using to test your hypothesis and the degrees of freedom
of your test. No matter what test you use, the p-value always describes the same thing: 
\emph{How often you can expect to see a test statistic as extreme or more extreme than the one calculated from your test}

\hypertarget{probability-density-function}{%
\subsubsection{Probability Density
Function}\label{probability-density-function}}

It's a statistical measure used to gauge the likely outcome of a
discrete value. PDFs are plotted on a graph typically resembling a bell
curve, with the probability of the outcomes lying below the curve. A
probability density function describes a probability distribution for a
random, continuous variable. Its used to find the chances that the value
of a variable will occur within a range of values that you specify. More
specifically, A PDF is a function where its integral for an interval
provides the probability (percentage of chance) of a value occurring in
that said interval. A question that could be answered using PDF is:
``What are the chances that the next IQ score that you measure will fall
between 120 and 140?''.

A PDF in statistics, \emph{probability density} refers to the likelihood
of a value occurring within an interval length of one unit.

A probability density function (PDF) explains which values are likely to
appear in a data-generating process at any given time or for any given
draw.

A cumulative distribution function (CDF) instead depicts how these
marginal probabilities add up, Ultimately reaching 100\% of possible
outcomes.

\hypertarget{cumulative-distribution-function}{%
\subsubsection{Cumulative Distribution
Function}\label{cumulative-distribution-function}}

CDF is used to calculate the area under the curve to the left from a
point of interest. It is used to evaluate the accumulated probability.

Is a function that describes the probability that a continuous random
variable X with a given probability distribution will be found at a
value less than or equal to x.

\hypertarget{empirical-distribution-function}{%
\subsubsection{Empirical Distribution
Function}\label{empirical-distribution-function}}

An empirical distribution function (EDF) is a cumulative distribution
function (CDF) that is derived from the observed data in a sample. The
EDF is used to estimate the true underlying distribution of the
population from which the sample is drawn.

Suppose a given random sample of size \(n\) is \(x_1, ..., x_n\) and let
\(x_1 < x_2 < ... < x_n\) be the order statistics; suppose further that
de distribution of \(x\) if \(F(x)\). The \emph{empirical distribution
function (EDF)} is \(F_n(x)\) defined by:

\(F_n(x) = \frac{n \leq x}{n}; -\infty < x < \infty\)

More precisely, the definition is
\begin{gather}
  F_n(x) = \begin{cases}  & 0 \text{ if } x < x_1\\  & \frac{1}{n} \text{ if } x_1 \leq x <x_2 \\  & \frac{2}{n} \text{ if } x_2 \leq x < x_3 \\  & \vdots \\  & \frac{n-1}{n}\text{ if } x = x_(n-1) \leq x < x_n \\  & 1 \ \text{if} \ x \geq x_n \end{cases}
\end{gather}

Thus \(F_n(x)\) is a step function, calculated from the data; as \(x\)
increases it takes a step up of height \(\frac{1}{n}\) as each sample
observation is reached.


\hypertarget{kurtosis}{%
\subsubsection{Kurtosis}\label{kurtosis}}

Kurtosis is a statistical measure that describes the ``tailedness'' of
the probability distribution of a random variable. It measures the
extremity of deviations or outliers, not the configuration of data near
the mean. Distribution with higher kurtosis has more extreme values in
the tails of the distribution, while a distribution with lower kurtosis
has a more uniform distribution of values.

% Todo
\hypertarget{degrees-of-freedom.}{%
\subsubsection{Degrees of
Freedom.}\label{degrees-of-freedom.}}

% Todo
\hypertarget{confidence-intervals.}{%
\subsubsection{Confidence Intervals.}\label{confidence-intervals.}}

\hypertarget{statistical-tests}{%
\subsection{Statistical Tests}\label{statistical-tests}}

Is an assumption about a population which may or may not be true.
Hypothesis testing is a set of formal procedures used by statisticians
to either accept or reject statistical hypotheses.

\hypertarget{t-test}{%
\subsubsection{T-Test}\label{t-test}}

A t-test is a statistical method used to determine whether two groups of
data have different means.

A t-test looks at the average of each group and how much variation there
is within each group, then compares these numbers to calculate a
t-value. If the t-value is large, it means that the difference between
the groups is likely not due to chance, and we can conclude that there's
a significant difference between them.

\href{https://www.youtube.com/watch?v=2ARvj-8tJBs}{Here's good video explaing this.}

A T-test: 
\begin{itemize}
  \item Assumes the null hypothesis is true and then evaluates whether is that is a bad assumtion.
  \item The p-value tells us the probability that the two datasets would have differed
        due to random chance under the assumption of the null hypothesis.
  \item If \(p < 0.05\), you can reject the null hypothesis. This corresponds to a 95 \% confidence interval
  \item A T-tes can be paired (you tested the same group twice) or unpaired (you tested two different groups).
\end{itemize}


Depending on the t-test, the test can determine whether:
\begin{itemize}
  \item One mean is different from a hypothesized value
  \item Two group means are different
  \item Paired means are different.
\end{itemize}

\hypertarget{one-sample-t-test.}{%
\paragraph{One-Sample T-test.}\label{one-sample-t-test.}}

Use a one-sample t-test to compare your sample mean to a hypothesized
value for the population and to create a confidence interval of likely
values for the population mean.

In example, you use the one-sample t-test when one group is compared against
a standard value, like the acidity of a liquitd to a neutral pH of 7. 

The formula for a one sample t-test: 
\begin{gather}
  t = \frac{\bar x - \mu_0}{s / \sqrt n}
\end{gather}

Where:
\begin{itemize}
  \item \(\mu_9 = \) The hypothesized mean. 
  \item \(\bar x = \) Is the sample mean. 
  \item \(s =\) The sample standard deviation. 
  \item \(n =\) The sample size. 
\end{itemize}

Assumptions: You have a random sample - Your data must be
continuous - Your sample data should follow a normal distribution or
have more than 20 observations.

\hypertarget{two-sample-t-test.}{%
\paragraph{Two-Sample T-test.}\label{two-sample-t-test.}}

You use the two-sample t-test when the two groups you're studying come from two
different populations.

The formula for the two-sample t-test:
\begin{gather}
  t = \frac{\bar x_1 - \bar x_2}{\sqrt{s_p \left (\frac{1}{n_1} + \frac{1}{n_2}\right )}}
\end{gather}

Where:
\begin{itemize}
  \item \(t = \) The t value. 
  \item \(x_1\) and \(x_2\) are the means of the two groups being compared. 
  \item \(s_p = \) The pooled standard error of the two groups. 
\end{itemize}

\hypertarget{paired-t-tests}{%
\paragraph{Paired t-tests}\label{paired-t-tests}}

Use paired t-tests to assess dependent samples, which are two
measurements on the same population, an example would be measuring 
before and after an experimental treatment.

\href{https://libguides.library.kent.edu/spss/pairedsamplesttest}{TODO}

Assumptions: Dependent Samples Unlike two-sample t-test,
paired t-test use the same people or items in both groups. One way to
determine whether a paired t-test is appropriate for your data is if
each row in the dataset corresponds to one person or item.



\hypertarget{z-test}{%
\subsubsection{Z-Test}\label{z-test}}

A z-test is a statistical hypothesis test that is used to determine
whether a sample mean is significantly different from a population mean,
when the population standard deviation is known.

The z-test is based on the standard normal distribution. The test
statistic is calculated by subtracting the population mean from the
sample mean, and dividing by the standard error of the mean.

\hypertarget{one-sample-z-test.}{%
\paragraph{One-Sample Z test.}\label{one-sample-z-test.}}

\begin{itemize}
\tightlist
\item
  \(H_0:\) The population mean equals a hypothesized vale
  \(\mu = \mu_0\).
\item
  \(H_A:\) The population mean DOES NOT equal a hypothesized value
  \(\mu \neq  \mu_0\)
\end{itemize}

When the p-value is less or equal to your significance level, reject the
null hypothesis.
\begin{gather}
  Z = \frac{\bar{x} - \mu_0}{\frac{\sigma}{\sqrt{n}}}
\end{gather}

\hypertarget{two-sample-z-test}{%
\paragraph{Two-Sample Z test}\label{two-sample-z-test}}

\begin{itemize}
\tightlist
\item
  \(H_0:\) Two population means are equal \(\mu_1 = \mu_2\).
\item
  \(H_A:\) Two popularion means are not equal \(\mu_1 \neq \mu_2\)
\end{itemize}

Again, if the p-value is less than or equal to your significance level,
reject the null hypothesis.

\begin{gather}
  Z = \frac{\bar{x}_1 - \bar{x}_2}{\sqrt{\frac{\sigma^2_1}{n_1} + \frac{\sigma^2_1}{n_2}}}
\end{gather}

\hypertarget{z-test-vs-t-test}{%
\paragraph{Z-Test vs T-test}\label{z-test-vs-t-test}}

Z-test require you to know the population standard deviation, while
t-test use a sample estimate of the standard deviation. In practice,
analysts rarely use the Z-test because it's rare that they'll know the
population standard deviation. It's even rarer that they'll know it and
yet need to assess an unknown population mean.

\textbf{When to use each?} When you know the population standard
deviation, use a Z-test. When you have a sample estimate of the standard
deviation, the best practice is to use a t-test, regardless of the
sample size.

\hypertarget{normality-test-and-goodness-of-fit}{%
\subsubsection{Normality Test and Goodness of
Fit}\label{normality-test-and-goodness-of-fit}}

In statistics normality testes are used to determine if a data set is
well-modeled by a normal distribution and compute how likely it is for a
random variable underlying the data set to be normally distributed.

Goodness of Fit is a statistical Hypothesis used to see how closely
observed data mirrors expected data. Goodness-of-Fit test can help
determine if a sample follows a normal distribution, if categorical
values are related, or if random samples are from the same distribution.

In assessing whether a given distribution is suited to a data-set, the
following tests and their underlying measures of fit can be used: -
Bayesian information criterion - Kolmogorov--Smirnov test - CramÃ©r--von
Mises criterion - Anderson--Darling test - Shapiro--Wilk test -
Chi-squared test - Akaike information criterion - Hosmer--Lemeshow test
- Kuiper's test - Kernelized Stein discrepancy - Zhang's ZK, ZC and ZA
tests - Moran test - Density Based Empirical Likelihood Ratio tests

\hypertarget{chi-square-test}{%
\subsubsection{Chi-Square Test}\label{chi-square-test}}

A chi-square test is a statistical hypothesis test that is used to
compare observed and expected results. It is used to determine whether
there is a significant association between two categorical variables.

It is typically used to test for independence between two variables,
which means that there's no relationship between them

The test is based on the chi square statistic, which is calculated by
comparing the observed frequencies of the data with the expected
frequencies under the null hypothesis.

If the difference between the observed and expected frequencies is large
enough, the test will reject the null hypothesis

\hypertarget{kolmogorov-smirnov-test-k-s-test-ks-test}{%
\subsubsection{Kolmogorov-Smirnov test (K-S Test \textbar{} KS
Test)}\label{kolmogorov-smirnov-test-k-s-test-ks-test}}

Its a test for normality. Its a non-parametric test of the equality of
continuous, one-dimensional probability distributions that can be used
to compare a sample with a reference probability distribution. Or to
compare two samples. The test is usually recommended for large samples
over 2000. For smaller samples, use Shapiro-Wilk

The test compares your data with a known distribution and lets you know
if they have the same distribution. Although the test is non-parametric
it doesn't assume any particular underlying distribution. It is commonly
used as a test for normality to see if your data is normally
distributed. It's also used to check the assumption of normality in
Analysis of Variance.

\hypertarget{shapiro-wilk-test}{%
\subsubsection{Shapiro-Wilk Test}\label{shapiro-wilk-test}}

The Shapiro-Wilk test is a way to tell if a random sample comes from a
normal distribution. The test gives you a W value; small values indicate
your sample is \emph{not} normally distributed. The formula for the W
value is: 

\begin{gather}
  W=\frac{\left (\sum_{i=1}^n a_i x_i \right )^2}{(\sum_{i=1}^n (x_i - \overline{x}))^2}
\end{gather}

Where: 
\begin{itemize}
  \item \(X_i\) are the oredered random sample values
  \item \(a_i\) are constants generated from the covariances, variances and means of the
  sample (size n) from a normally distributed sample.
\end{itemize}

The test has limitations, most importantly that it has a bias by sample
size. The larger the sample, the more likely you'll get a statistically
significant result.

\hypertarget{anderson-darling}{%
\subsubsection{Anderson-Darling}\label{anderson-darling}}

The Anderson-Darling test is a statistical test used to determine
whether a given data set is drawn from a specific probability
distribution, such as the normal distribution or exponential
distribution. It is a goodness-of-fit test that is based on the distance
between the empirical distribution function (EDF) of the sample and the
cumulative distribution function (CDF) of the theoretical distribution
being tested. The test calculates a test statistic, which is a weighted
sum of the squared differences between the EDF and the CDF, with greater
weight given to the tails of the distribution.

The Anderson-Darling returns an statistic called \emph{Anderson-Darling
statistic (AD)}: The Anderson-Darling statistic is the \emph{test
statistic}. It's like the t-value for the t-test or the F-value for the
F-test. Typically you don't interpret this statistic directly, but the
software uses it to calculate the p-value for the test.

The hypotheses for the Anderson-Darling test are:
\begin{itemize}
  \item \(H_0\): The data follow a specified distribution.
  \item \(H_A\): The data do not follow a specified distribution.
\end{itemize}


The formula for the Anderson-Darling
\hypertarget{analysis-of-variance}{%
\subsection{Analysis of Variance}\label{analysis-of-variance}}
I've decided that Analysis of Variance should have its own chapter.


% TODO: 
\hypertarget{statistical-plots}{%
\subsection{Statistical Plots}\label{statistical-plots}}

\hypertarget{stem-and-leaf-plot}{%
\subsubsection{Stem-and-Leaf Plot}\label{stem-and-leaf-plot}}

\hypertarget{normal-probability-plot-q-q-plot}{%
\subsubsection{Normal Probability Plot (Q-Q
Plot)}\label{normal-probability-plot-q-q-plot}}

The normal probability plot is a graphical technique for assessing
whether or not a data set is approximately normally distributed. The
data are plotted against a theoretical normal distribution in such a way
that the points should form an approximate straight line. Departures
from this straight line indicate departures from normality.

Usually, the Normal Probability Plot its accompanied by a Normality
Test, often the Anderson-Darling or the Shapiro-Wilk tests\\
To determine whether the data is normal, compare the p-value to the
significance level \(\alpha\). \(H_0:\) The data follows is normally
distributed \(H_A:\) The data does not follow a normal distribution

\begin{itemize}
\tightlist
\item
  P-value \(\leq \alpha\): The data do not follow the normal
  distribution (reject \(H_0\)) If the p-value is less than or equal to
  the significance level, the decision is to reject the null hypothesis
  and conclude that your data does not follow a normal distribution.
\item
  P-value \(> \alpha\): Cannot conclude the data do not follow the
  distribution (Fail to reject \(H_0\)) If the p-value is larger than
  the significance level, the decision is to fail to reject the null
  hypothesis because you do not have enough evidence to conclude that
  your data do not follow the distribution. However, you cannot conclude
  that the data do follow the distribution.
\end{itemize}

% TODO
\hypertarget{p-p-plot}{%
\subsubsection{P-P Plot}\label{p-p-plot}}

% TODO
\hypertarget{kernel-density-estimation}{%
\subsubsection{Kernel density estimation}\label{kernel-density-estimation}}

% TODO
\hypertarget{correlation-and-association}{%
\subsection{Correlation and Association}\label{correlation-and-association}}

% TODO
\hypertarget{pearson-correlation-coefficient}{%
\subsubsection{Pearson Correlation Coefficient}\label{pearson-correlation-coefficient}}

% TODO
\hypertarget{spearman-rank-correlation-coefficient}{%
\subsubsection{Spearman Rank Correlation Coefficient}\label{spearman-rank-correlation-coefficient}}

% TODO
\hypertarget{covariance}{%
\subsubsection{Covariance}\label{covariance}}

% TODO
\hypertarget{time-series-analysis}{%
\subsection{Time Series Analysis}\label{time-series-analysis}}

% TODO
\hypertarget{moving-averages}{%
\subsubsection{Moving Averages}\label{moving-averages}}

% TODO
\hypertarget{exponential-smoothing}{%
\subsubsection{Exponential Smoothing}\label{exponential-smoothing}}

    

    \hypertarget{dr.-w.-edwards-deming}{%
\section{Dr.~W. Edwards Deming}\label{dr.-w.-edwards-deming}}

    William Edwards Deming (1900-1993) was an American statistician,
engineer, and management consultant who is widely regarded as one of the
leading thinkers in the field of quality control and management. Deming
is best known for his work in Japan after World War II, where he helped
to revolutionize Japanese manufacturing and transform it into a world
leader in quality and productivity.

Recommended material: - Out of the Crisis

\hypertarget{points-for-management}{%
\subsection{14 Points for Management}\label{points-for-management}}

\hypertarget{create-constancy-of-purpose}{%
\subsubsection{1. Create constancy of
purpose:}\label{create-constancy-of-purpose}}

Organizations should have a long-term focus and strive to achieve their
core mission.

Examples: - Commit to developing your team on an ongoing basis. - Create
a vision of the future. Make sure evey employee knows this vision. - Ask
for process improvements and product quality improvements every week
from your team

\hypertarget{adopt-the-new-philosophy}{%
\subsubsection{2. Adopt the new
philosophy:}\label{adopt-the-new-philosophy}}

Organizations must shift from a focus on short-term profits to a focus
on continuous improvement and customer satisfaction. You must no longer
tolerate commonly accepted levels of mistakes, people who don't know
what they're doing, defects and inadequate supervision. At the heart of
this new philosophy should be the desire to put your customer's needs
first.

Examples: - If you run a restaurant, is it acceptable for a customer to
order only to be later told that what they ordered is out of stock? - Is
it acceptable to wait over fifteen minutes to speak to a customer
suppoert team member if you run a bank?

\hypertarget{cease-dependence-on-inspection}{%
\subsubsection{3. Cease dependence on
inspection:}\label{cease-dependence-on-inspection}}

Quality should be built into the product or service, rather than relying
on inspection to catch defects. Stop depending on inspections to improve
quality and build quality into your processes instead. An inspection
doesn't improve quality because it happens too late. At the point of
inspection, the quality of the product already exists, good or bad, so
all an inspection does is find an existing lack of quality.

\hypertarget{end-the-practice-of-awarding-business-on-price-alone}{%
\subsubsection{4. End the practice of awarding business on price
alone:}\label{end-the-practice-of-awarding-business-on-price-alone}}

Suppliers should be chosen based on their ability to provide
high-quality products or services.

\hypertarget{improve-constantly-and-forever-the-system-of-production-and-service}{%
\subsubsection{5. Improve constantly and forever the system of
production and
service:}\label{improve-constantly-and-forever-the-system-of-production-and-service}}

Organizations should continually strive to improve their processes,
products, and services.

\hypertarget{institute-training}{%
\subsubsection{6. Institute training:}\label{institute-training}}

Employees should be trained in the skills and knowledge necessary to
perform their jobs effectively.

\hypertarget{institute-leadership}{%
\subsubsection{7. Institute leadership:}\label{institute-leadership}}

Management should provide clear direction and guidance, and lead by
example.

\hypertarget{drive-out-fear}{%
\subsubsection{8. Drive out fear:}\label{drive-out-fear}}

Employees should be encouraged to speak up and provide feedback without
fear of retribution.

\hypertarget{break-down-barriers-between-departments}{%
\subsubsection{9. Break down barriers between
departments:}\label{break-down-barriers-between-departments}}

Departments should work together and communicate effectively to achieve
common goals.

\hypertarget{eliminate-slogans-exhortations-and-targets-for-the-workforce}{%
\subsubsection{10. Eliminate slogans, exhortations, and targets for the
workforce:}\label{eliminate-slogans-exhortations-and-targets-for-the-workforce}}

Management should focus on providing the resources and support necessary
for employees to perform their jobs effectively.

\hypertarget{eliminate-numerical-quotas-for-the-workforce-and-numerical-goals-for-management}{%
\subsubsection{11. Eliminate numerical quotas for the workforce and
numerical goals for
management:}\label{eliminate-numerical-quotas-for-the-workforce-and-numerical-goals-for-management}}

Performance should be evaluated based on overall improvement, rather
than meeting specific targets.

\hypertarget{remove-barriers-to-pride-of-workmanship}{%
\subsubsection{12. Remove barriers to pride of
workmanship:}\label{remove-barriers-to-pride-of-workmanship}}

Employees should take pride in their work and be given the opportunity
to contribute to the success of the organization.

\hypertarget{institute-a-vigorous-program-of-education-and-self-improvement}{%
\subsubsection{13. Institute a vigorous program of education and
self-improvement:}\label{institute-a-vigorous-program-of-education-and-self-improvement}}

Employees should be given opportunities for personal and professional
development.

\hypertarget{put-everybody-in-the-company-to-work-to-accomplish-the-transformation}{%
\subsubsection{14. Put everybody in the company to work to accomplish
the
transformation:}\label{put-everybody-in-the-company-to-work-to-accomplish-the-transformation}}

All employees should be involved in the process of continuous
improvement and strive to achieve the organization's goals

\hypertarget{five-deadly-diseases}{%
\subsection{Five Deadly Diseases}\label{five-deadly-diseases}}

\hypertarget{lack-of-constancy-of-purpose}{%
\subsubsection{Lack of constancy of
purpose:}\label{lack-of-constancy-of-purpose}}

Organizations without a clear and consistent mission, vision, and set of
values can become easily distracted and lose focus.

\hypertarget{emphasis-on-short-term-profits}{%
\subsubsection{Emphasis on short-term
profits:}\label{emphasis-on-short-term-profits}}

A focus on short-term profits can lead to cutting corners, sacrificing
quality, and neglecting long-term planning and investment.

\hypertarget{evaluation-by-performance-merit-rating-or-annual-review}{%
\subsubsection{Evaluation by performance, merit rating, or annual
review:}\label{evaluation-by-performance-merit-rating-or-annual-review}}

Traditional performance evaluation systems that rely on numerical
ratings or annual reviews can be arbitrary and demotivating, leading to
a focus on meeting targets rather than improving performance.

\hypertarget{mobility-of-management}{%
\subsubsection{Mobility of management:}\label{mobility-of-management}}

Frequent turnover or reshuffling of management can disrupt established
processes, relationships, and communication channels, leading to
inefficiency and confusion.

\hypertarget{running-a-company-on-visible-figures-alone}{%
\subsubsection{Running a company on visible figures
alone:}\label{running-a-company-on-visible-figures-alone}}

Relying solely on quantitative data can lead to a narrow focus on easily
measurable outcomes, ignoring the importance of qualitative factors such
as customer satisfaction, employee morale, and innovation.

\hypertarget{common-causes-and-specials-causes-of-improvement.}{%
\subsection{Common Causes and Specials Causes of
Improvement.}\label{common-causes-and-specials-causes-of-improvement.}}

The central problem in management and in leadership, is failure to
understand the information in variation.

A distribution only presents accumulated history of performance of a
process, nothing about its capability. \textbf{A process only has a
capability if it is stable}

There are two kind of mistakes when it comes to variaton: 1. Ascribe a
variation or mistake to a special cause when in fact the cause belongs
to the system. 2. Ascribe a variation or a mistake to the system when in
fact the cause was special.

Overadjustment is a common example of mistake No.~1. Never doing
anything to try to find a special cause is a common exmaple of mistake
No.~2.

Supervisors commonly make the mistake of overadjustment when they direct
to the attention of one of their people any mistake or defect, without
first ascerting that the worker was actually responsible for the
mistake. Did the worker make the mistake, or was the system responsible
for it?

Difference between conformance to specifications and statistical process
control. - The aim in production should not just to get statistical
control, but to shrink variaton. Costs go down as variation is reduced.
It is not enough to meet specifications.

\hypertarget{important-concepts}{%
\subsection{Important concepts}\label{important-concepts}}

System: A system is a network of interdependent components that work
together to achieve a common goal or purpose. In his view, a system
includes not only the physical components of a process, but also the
people, procedures, and culture that make up the organization.

Stable System: A stable system is one where the process is in control
and variation is due to common causes. In a stable system, the variation
can be predicted and controlled using statistical process control (SPC)
techniques.

Common Causes: Common causes of variation are inherent in the system and
cannot be eliminated. They are predictable and can be managed through
process improvement and statistical analysis.

Special Causes: Special causes of variation are due to factors outside
the system, such as machine malfunctions, operator errors, or changes in
the environment. These causes are unpredictable and require
investigation and corrective action to eliminate.

Control Charts: Control charts are a tool for monitoring and controlling
a process. They allow you to distinguish between common and special
causes of variation, and to determine when the process is out of
control.

Action on Common Causes: When common causes of variation are identified,
they can be addressed through process improvement efforts, such as
training, standardization, or redesign of the process.

Action on Special Causes: When special causes of variation are
identified, they require immediate corrective action to eliminate the
cause and prevent future occurrences.

    \hypertarget{statistical-process-control}{%
\section{Statistical Process
Control}\label{statistical-process-control}}

    \hypertarget{rational-sub-grouping}{%
\subsection{Rational Sub-grouping}\label{rational-sub-grouping}}

\hypertarget{control-charts}{%
\subsection{Control Charts}\label{control-charts}}

\hypertarget{i-mr-charts}{%
\subsubsection{I-MR Charts}\label{i-mr-charts}}

\hypertarget{x-barr-chart}{%
\subsubsection{X-Bar/R Chart}\label{x-barr-chart}}

\hypertarget{other-charts}{%
\subsubsection{Other Charts}\label{other-charts}}

TODO. This may be wrong. 
\hypertarget{capability-analysis}{%
\subsection{Capability Analysis}\label{capability-analysis}}

\hypertarget{different-standard-deviations.}{%
\paragraph{Different Standard deviations used.}\label{different-standard-deviations.}}

The calculation and definition of the different standard deviations in
this sections is based upon what Minitab has in its documentation about
Capability Analysis.

\hypertarget{definitions}{%
\paragraph{Definitions}\label{definitions}}

\begin{center}
  
% \begin{tabular}{@{}lcr@{}}
\begin{tabular}{l l l}
  \toprule
  \textbf{Symbol}  & \textbf{Meaning} \\
  \midrule
  \( T \)                         & \(=\) target \\
  \( LSL \)                       & \(=\) lower specification limit \\
  \( USL \)                       & \(=\) upper specification limit \\
  \( \mu \)                       & \(=\) process mean \\
  \( tol \)                       & \(=\) sigma tolerance \\
  \( m \)                         & \(=\) midpoint of LSL and USL \\
  \( \bar x \)                    & \(=\) estimate of process mean \\
  \( \sigma_{Within} \)           & \(=\) within subgroup process standard deviation \\
  \( \hat \sigma_{Within} \)      & \(=\) estimate of within subgroup process standard deviation \\
  \( n \)                         & \(=\) total number of observations \\
  \( n_i \)                       & \(=\) number of observations in subgroup \(i\) \\
  \( C_4(n_i) \)                  & \(=\) unbiasing constant for subgroups of size \(n_i\) (for use with sample standard deviations)\\
  \( d_2(n_i) \)                  & \(=\) unbiasing constant for subgroups of size \(n_i\) (for use with sample standard deviations)\\
  \( \sigma_{Overall} \)          & \(=\) overall process standard deviation \\
  \( \hat \sigma_{Overall} \)     & \(=\) estimate of the overall process standard deviation \\
  \( P(X) \)                      & \(=\) probability of event \(X\) \\
  \( Z \)                         & \(=\) standard normal variable \\
  \( x_i \)                       & \(=\) the \(i\)th observation \\
  \( x_{ij} \)                    & \(=\) the \(j\)th obersvation of the \(i\)th subgroup \\
  \bottomrule
\end{tabular}
\end{center}



\hypertarget{within-standard-deviation-sigma_within}{%
\subsubsection{Within Standard Deviation \texorpdfstring{$\sigma_{\mathrm{within}}$}{sigma within}}\label{within-standard-deviation-sigma_within}}


It's an estimate of the variation within the subgroups. If your data is
collected properly, the within-subgroup variation should not be
influenced by changes to the process inputs. such as tool wear or
different lots of material. In that case, the within standard deviation
represents the natural and inherit variation of the process over a short
period of time. \textbf{It indicates the potential variation of the
process if shifts and drifts between groups were eliminated}
Within-subgroup standard deviation is also used when the sample size =
1, the formulas for sample size \textgreater{} 1 and sample size = 1 are
different.

\hypertarget{subgroup-size-1}{%
\paragraph{Subgroup Size = 1}\label{subgroup-size-1}}

When the subgroup size = 1, you can estimate the \(\sigma_{within}\)
using one of the following methods:

\hypertarget{average-moving-range}{%
\subparagraph{Method 1: Average of Moving Range}\label{average-moving-range}}

\begin{gather}
  \sigma_{\bar x} = \frac{\bar R}{d_2(w)}  
\end{gather}

Where: 
\begin{itemize}
  \item \(\bar R =\) The average moving range is the average value of the
  moving range of two or more consecutive points.
  \item \(d_2 =\) An unbiasing constant read from a table 
  \item \(w =\) The number of observations used in the moving range.
\end{itemize}

Now, the calculation of the moving range (\(MR\)) is not straightforward, it depends
in the window size, in order to calculate the Moving Range with 
a window size \(w\) you have the equation:

\begin{gather}
  MR_i = | \text{Max}[x_{i}, ...,  x_{i-w+1}] - \text{Min}[X_i, ..., X_{i-w+1}]| \ \ \text{for} \ i=w, \ ..., \ n
\end{gather}

This equation represents the calculation of the range of a sliding
window over a sequence of numbers. The sequence of numbers is
represented by the vector \(X = [x_1, x_2, ..., x_n]\). The window size is
represented by the parameter \(w\), which specifies the number of
elements to include in each window. The equation calculates the
difference between the maximum and minimum values within each sliding
window of size w, and stores the resulting range values in a new vector
\(MR\)

A more simpler form of the equation is presentented when the value for \(w\) is \(2\), which
is usually the most common case for the Moving Range. 

\begin{gather}
  MR_i = |x_i - x_{i-1}| \ \ \text{for} \ \ i=2,3,..., n
\end{gather}


Then, calculate the average of the moving range:
\begin{gather}
   \overline{MR} = \frac{R_w + ... + R_n}{n-w+1}  
\end{gather}


This equation calculates the average range over all windows of size w in
a sequence of numbers. It uses the range vector \(R\) calculated by the
previous equation, where \(R_i\) represents the range of the i-the window
of size w.

\hypertarget{median-moving-range}{%
\subparagraph{Method 2: Median of Moving Range}\label{median-moving-range}}

\begin{gather}
  \sigma_{\bar x} = \frac{\widetilde{MR}}{d_4(w)}  
\end{gather}

Where: 
\begin{itemize}
  \item \(\widetilde{MR} =\) Median of the Moving Range. 
  \item \(w\) =  The number of observations used in the moving range. (AKA. The window size. Default is 2) 
\end{itemize}

The median moving range is the median value of the moving range of two
or more consecutive points. Use this method when the data have extreme
ranges that influence average of the moving ranges

\hypertarget{todo.-subgroup-size-1}{%
\subparagraph{Subgroup Size \textgreater{} 1}\label{todo.-subgroup-size-1}}
To estimate \(\sigma_{within}\) for subgroups of size bigger than one. You could use
one of three different methods: 


\hypertarget{pooled-standard-deviation}{
\subparagraph*{Method 1: Pooled Standard Deviation}\label{pooled-standard-deviation}}
A pooled standard deviation is just a weighted average of the variances
from two or more groups of data when they are assumed to come from populations 
with a common variance.

The formula to obtain the unbiased estimator of \(\sigma_{Within}\) is given by:

\begin{gather}
  \sigma_{Within} = \frac{S_P}{C_4 (d+1)}
\end{gather}

Where the \(S_P\) is the Spooled Standard Deviation, and is given by: 

\begin{gather}
  S_P = \sqrt{\frac{\sum_i \sum_j (x_{ij} - \bar x_i)^2}{\sum_i(n_i - 1)}}
\end{gather}

Where: 
\begin{itemize}
  \item \(d\) = Degrees of freedom for \(S_p = \sum (n_i - 1)\).
  \item \(x_{ij}\) = \(j^{th}\) observation in the \(i^{th}\) group. 
  \item \(C_4(d+1)\) = Unbiasing constant.
  \item \(\Gamma\) = Gamma function.
\end{itemize}


% TODO. 
\hypertarget{average-of-subgroup-ranges}{
\subparagraph*{Method 2: Average of Subgroup Ranges}\label{average-of-subgroup-ranges}}
  
  % TODO. 
\hypertarget{average-of-subgroup-standard-deviations}{
\subparagraph*{Method 3: Average of Subgroup Standard Deviations}\label{average-of-subgroup-standard-deviations}}


\hypertarget{overall-standard-deviation-sigma_overall}{%
\subsubsection{Overall Standard Deviation \(\sigma_{overall}\)}\label{overall-standard-deviation-sigma_overall}}
Is the standard deviation of all the measurements and is an
estimate of the overall variation of the process. If your data are
collected properly, the overall standard deviation captures all sources
of systemic variation. In that case, it represents the \emph{actual}
variation of the process that the customer experiences over time.
\textbf{It is used to calculate the PP and PPK values, and other
measures of the overall capability of the process}

Unbiased estimator of \(\sigma_{overall}\)

\begin{gather}
  \sigma_{overall}= \frac{S}{C_4(N)}
\end{gather}

% Todo. Explain further
\(S\) being the Standard Deviation for all samples with subgroups:
\begin{gather}
  S = \sqrt{\frac{\sum_i \sum_j (x_{ij} - \bar{x})^2}{(\sum n_i) - 1}}
\end{gather}

Where: 
\begin{itemize}
  \item \(x_{ij} = \) The \(j^{th}\) observation of the \(i^{th}\) subgroup 
  \item  \(\bar x =\) Process mean 
  \item \(n_i = \) Number of observation in the \(i^{th}\) subgroup.
  \item \(C_4(N) = \) Unbiasing constant. 
  \item \(N = \sum n_i =\) Total number of observations.
\end{itemize}


\emph{Note, it is usually best practice to not use the unbiasing
constant when estimating \(\sigma_{overall}\), you should estimate
\(\sigma_{overall}\) by \(S\) directly.}

If there are no subgroups, then the formula for \(S\) is just:
\begin{gather}
  S = \sqrt{\frac{\sum^N_{i=1} (x_i - \bar x)^2}{N-1}}
\end{gather}

Where: \(S\) = Sample standard deviation \(N\) = The number of
observations \(x_i\) = The observed values of a sample item and
\(\overline {x}\) = The mean value of the observations.

\hypertarget{between-standard-deviation}{%
\subsubsection{Between Standard
Deviation}\label{between-standard-deviation}}

Is an estimate of the variation between the subgroups. For example, if
each subgroup is collected from a different batch of items, large
between-subgroup standard deviation indicates a large amount of
variability between the items in different batches. \textbf{It is used
calculate the between/within subgroup variation}

\hypertarget{bw-standard-deviation}{%
\subsubsection{B/W Standard Deviation}\label{bw-standard-deviation}}

Is a single value that includes both the variation between subgroups and
the variation within subgroups. It's the square root of the sum of the
between-subgroup variance and the within-subgroup-variance. \textbf{It
is used calculate CP, CPK and other measures of the between/within
capability of a process}

\hypertarget{capability-indices}{%
\subsubsection{Capability Indices}\label{capability-indices}}

\hypertarget{cp}{%
\paragraph{CP}\label{cp}}

CP is a measure of the potential capability of the process. It is a
ratio comparing two vales. The specification spread \((USL - LSL)\) - The
spread of the process based on the standard deviation. - \textbf{Cp
evaluates potencial capability based on the variation in your process,
not its location}

\begin{gather}
  Cp = \frac{USL - LSL}{TOL \times \hat{\sigma}_{within}}
\end{gather}

Where: 
\begin{itemize}
  \item \(USL =\) Upper specification limit
  \item \(LSL =\) Lower specification limit
  \item \(TOL =\) Multiplier of the sigma tolerance \emph{Usually six}
  \item \(\hat{\sigma}_{within} =\) Within  standard deviation
\end{itemize}

Interpretation Because CP doesn't consider the location of the process,
it indicates the potencial capability that your process could achieve if
it were centered.

\hypertarget{cpl}{%
\paragraph{CPL}\label{cpl}}

It's a measure of the potencial capability of the process based on its
LSL. CPL is a ratio that compares two values - The distance from the
process mean to the LSL - The one-sided spread of the process based on
the within subgroup standard deviation.

Because CPl considers both the process mean and the process spread, it
evaluates both the location and the variation of th eprocess.

\begin{gather}
  CPL = \frac{\bar{x} - LSL}{(\frac{TOLER}{2})\hat{\sigma}_{within}}
\end{gather}

\hypertarget{cpu}{%
\paragraph{CPU}\label{cpu}}

It's a measure of th epotencial capability of the process based on its
upper specificaction limit.Compares two values: - The distance from the
process mean to the USL - The one-sided spread of the process based on
the variation within the subgroups.

\begin{gather}
  CPU = \frac{USL - \bar{x} }{(\frac{TOL}{2})\hat{\sigma}_{within}}
\end{gather}

\hypertarget{cpk}{%
\paragraph{CPK}\label{cpk}}

It's a measure of the potencial capability of the process and equals to
the minimum of the CPU and CPL. Use CPK to evaluate the potential
capability of your process based on both the process location and the
process spread. Potential capability indicates the capability that could
be achieved if the process shifts and drifts were eliminated.
\begin{gather}
  CPK = min(CPU, CPL)
\end{gather}

\href{https://www.six-sigma-material.com/Cpk.html}{For more information click here.}

\hypertarget{pp}{%
\paragraph{PP}\label{pp}}

It's a measure of the overall capability of the process. PP is a ratio
that compares two values. - The specification spread (USL - LSL) - The
spread of the process based on the overall standard deviation.

It evaluates overall capability based on the variation in the process,
but not its location.



\textbf{Where:} - \(USL =\) Upper specification limit - \(LSL =\) Lower
specification limit - \(TOL =\) Multiplier of the sigma tolerance

standard deviation

\hypertarget{todo.-ppu}{%
\paragraph{Todo. PPU}\label{todo.-ppu}}
\begin{gather}
  PPU = \frac{USL - \mu }{(\frac{TOL}{2})\hat{\sigma}_{overall}}
\end{gather}

\hypertarget{todo.-ppl}{%
\paragraph{Todo. PPL}\label{todo.-ppl}}

\begin{gather}
  PPL = \frac{\mu - LSL}{(\frac{TOL}{2})\hat{\sigma}_{overall}}
\end{gather}

\hypertarget{todo.-ppk}{%
\paragraph{Todo. PPK}\label{todo.-ppk}}

It's a measure of the overall capability of the process and its equals
to the minimum of the PPU and PPL
\begin{gather}
  PPK = min(PPU, PPL)
\end{gather}

\hypertarget{ppm}{%
\paragraph{PPM}\label{ppm}}

PPM is the expected number of parts per million that are outside of the
specification limits, it's based on the overall variation of the
process.

\begin{gather}
  [PPM < LSL(\text{Exp. Overall})] + [PPM > USL(\text{Exp. Overall})] 
  \\ = \left (1,000,000 \times \left [1 - \phi \left (\frac{\bar x - LSL}{S} \right ) \right ] \right ) + \left (1,000,000 \times \left [1 - \phi \left (\frac{USL - \bar x}{S} \right ) \right ] \right )
\end{gather}

This formula is used in statistical process control to estimate the number
of defective parts or products in a manufacturing process. It involves two main steps:

First, calculate the number of parts per million (PPM) that fall
below the lower specification limit (LSL) or above the upper
specification limit (USL), given the expected overall process
capability. This is done using the cumulative distribution function (CDF) of a standard normal distribution.

Second, add the two PPM values together to get the total estimated number of defective parts.

Here is a step-by-step explanation of the formula:

\begin{itemize}
  \item \(PPM < LSL(Exp. Overall)\) 
  \\ Represents the number of defective parts that fall below the lower specification limit (LSL) when the
  overall process is expected to be in control. 
  \\ The term ``Exp. Overall'' refers to the expected overall process capability, which is typically
  determined using historical data or engineering analysis.

  \item \(PPM > USL(Exp. Overall)\) 
  \\ Represents the number of defective parts that fall above the
  upper specification limit (USL) when the overall process is expected to be in control.

  \item \( 1,000,000 \times [1 - \phi ( (\bar x - LSL)/S) ]\)
  \\ This term calculates the PPM of defective parts that fall below the LSL. It uses the CDF of a standard normal distribution to find the proportion of the process data that fall below the expected LSL.
  \\ The term \( (\bar x - LSL)/S \) represents the number of standard deviations the process mean \(\bar x\) is below the LSL.
  
  \item \( 1,000,000 \times [1 - \phi ( (USL - \bar x )/S) ]\)
  \\ This term calculates the PPM of defective parts that fall above the USL. It uses the CDF of a standard normal distribution to find the proportion of the process data that fall above the expected USL.
  \\ The term \( (USl - \bar x)/S\) represents the number of standard deviations the process mean \(\bar x\) is above the USL. 

\end{itemize}
% Todo. I was here... 
Here is a step-by-step explanation of the formula:

\begin{itemize}
  \item \(PPM < LSL(Exp. Overall)\) 
  \\ Represents the number of defective parts that fall below the lower specification limit (LSL) when the
  overall process is expected to be in control. 
  \\ The term ``Exp. Overall'' refers to the expected overall process capability, which is typically
  determined using historical data or engineering analysis.

  \item \(PPM > USL(Exp. Overall)\) 
  \\ Represents the number of defective parts that fall above the
  upper specification limit (USL) when the overall process is expected to be in control.

  \item \( \frac{\bar x - LSL}{S} = Z_{LSL} \) 
  \\ \(Z_{LSL}\) Represents the number of standard deviations the process mean \(\bar x\) is below the lower specification limit (LSL)
  
  \item \( \frac{USL - \bar x}{S} = Z_{USL} \) 
  \\ \(Z_{USL}\) Irepresents the number of standard deviations the process mean \(\bar x\) is below the lower specification limit (LSL)  

  \item  \( \phi (\cdot) \) Is the Cumulative Distribution Function (CDF) of a standard normal distribution.
  \\ The CDF of the standard normal distribution is used to find the proportion of the process data that fall below the expected LSL
  \item  \( \phi (Z_{LSL}) \) The CDF of the standard normal distribution is used to find the proportion of the process data that fall below the expected LSL
  \item  \( \phi (Z_{USL}) \) The CDF of the standard normal distribution is used to find the proportion of the process data that fall above the expected USL
\end{itemize}

\hypertarget{resources}{%
\paragraph{Resources}\label{resources}}

https://support.minitab.com/en-us/minitab/21/help-and-how-to/quality-and-process-improvement/capability-analysis/how-to/capability-sixpack/normal-capability-sixpack/methods-and-formulas/within-capability/\#cp

https://support.minitab.com/en-us/minitab/21/help-and-how-to/quality-and-process-improvement/capability-analysis/how-to/capability-sixpack/normal-capability-sixpack/methods-and-formulas/overall-capability/\#cpm

https://support.minitab.com/en-us/minitab/20/help-and-how-to/quality-and-process-improvement/capability-analysis/how-to/capability-analysis/normal-capability-analysis/interpret-the-results/all-statistics-and-graphs/potential-within-capability/

https://www.six-sigma-material.com/Ppk.html

    

    \hypertarget{design-of-experiments}{%
\subsection{Design of Experiments}\label{design-of-experiments}}

\hypertarget{full-factorial-designs}{%
\subsubsection{Full Factorial Designs}\label{full-factorial-designs}}

\hypertarget{fractional-factorial-designs}{%
\subsubsection{Fractional Factorial
Designs}\label{fractional-factorial-designs}}

\hypertarget{response-surface-designs}{%
\subsubsection{Response Surface
Designs}\label{response-surface-designs}}

\hypertarget{statistical-process-monitoring}{%
\subsection{Statistical Process
Monitoring}\label{statistical-process-monitoring}}

\hypertarget{monitoring-process-stability}{%
\subsubsection{Monitoring Process
Stability}\label{monitoring-process-stability}}

\hypertarget{detecting-process-changes}{%
\subsubsection{Detecting Process
Changes}\label{detecting-process-changes}}

    \hypertarget{quality-costs}{%
\section{Quality Costs}\label{quality-costs}}

    Quality costs are a measure of the costs specifically assosiated with
the achievement or non achievement of product or service quality
including all product or service requirements established by the company
and its contracts with customers ans society.

Quality costs repreent the difference between the actual cost of a
product or service and what the reduced cost would be if there were no
possibility of substandard service, product failure or manufacturing
defects.

The most common format for categorizing quality costs is the Prevention
Appraisal-Failure model.

\begin{itemize}
\tightlist
\item
  \textbf{Prevention costs:} The cost of all activities specifically
  designed to prevent poor quality in products or services.
\item
  \textbf{Failure Costs:} Costs resulting from products or services not
  conforming to requirements or customer/user needs.
\item
  \textbf{Appraisal Costs:} Costs associated with measuring, evaluating,
  or auditing products or services to assure conformance to quality
  standards.
\item
  \textbf{Internal Failure Costs:} Failure costs occurring prior to
  delivery or shipment of the product.
\item
  \textbf{External Failure Costs:} Failure cost occurring after delivery
  or shipment of the product.
\item
  \textbf{Total Quality Costs:} The sum of the above costs, representing
  the difference between the actual cost of a product or service and
  what the reduced cost would be if there wer no defects.
\end{itemize}



The goal of any quality cost system is to facilitate quality improvement
efforts that will lead to operating cost reduction opportunities.

The strategy for using quality costs is quite simple: 1. Take direct
attack on failure costs. 2. Invest in the right prevention activities.
3. Reduce appraisal costs according to results achieved 4. Continuously
evaluate and redirect prevention efforts to gain further improvement.

This strategy is based on the premise that: - For each failure there's a
root cause. - Causes are preventable. - Prevention is always cheaper.

\hypertarget{taguchi-quality-loss-function-qlf-and-the-hidden-costs-of-quality}{%
\subsection{Taguchi Quality Loss Function (QLF) and the hidden costs of
quality}\label{taguchi-quality-loss-function-qlf-and-the-hidden-costs-of-quality}}

A quality characteristic is whatever we measure to judge performance.

The Quality Loss Function is used to estimate costs when the product or
process characteristics are shifted from the target value. This is
represented by the following equation:

\begin{gather}
  L(Y) = k(y-T)^2
\end{gather}

Where: \(L(Y)\) is the cost incurred when the characteristic \(y\) is
shifted from the target \(T\) and \(k\) is a constant depending on the
process.

Loss occurs not only when a product is outside the specifications, but
also when a product falls within the specifications. Although a loss
function may take on many forms, Taguchi has found that the simple
quadratic function approximates the behavior of loss in many instances.

Since the QLF curve is quadratic in nature, loss increases by the square
of the distance from the target value.

\hypertarget{quality-cost-bases}{%
\subsection{Quality Cost Bases}\label{quality-cost-bases}}

Actual dollars expended is usually the best indicator for determining
where quality improvement projects will have the greatest impact on
profit and where corrective action should be taken, but unless the
amount of work performed is relatively constant, it will not provide a
clear indication of uality cost improvement trends.

The prime value of a quality cost system is in identifying opportunities
for improvement and then providing a measurement of that improvement
over time.

Short-range bases should be directly related by time and location to
quality costs as they are being incurred and reported. They should
relate the cost of quality to the amount of work performed.

Typical examples include overalll operating costs, total or direct labor
costs, value added costs, and the actual average cost of delivered
product or service.

For current, ongoing applications, several bases can be used. The
following examples are typical indices that incorporate this feature: -
Internal failure costs as percent of total production. - External
failure costs as an average percent of net sales. - Procurement
appraisal costs as a percent of total purchased material. - Operations
appraisal costs as percent of total purchased material costs. - Total
quality costs as percent of production.

There is no single perfect base, each base can be misleading if used
alone and this can easily lead to confusion and disinterest. It's
important to the success of quality cost use that bases for individual
progressm easurements not appear unnatural to the functional area.
Instead, they should be seen as complementary, for example ``rework
costs as percent of area labor costs''.

\textbf{They could aslo be used to provide indices that may have shock
value} simply to get the corrective action juices flowing, for example
``Hey, did you know that for every dollar expended in your area, 50
cents is the cost of poor quality?''

\hypertarget{trend-analysis-and-the-improvement-process}{%
\subsection{Trend Analysis and the Improvement
Process}\label{trend-analysis-and-the-improvement-process}}

Quality costs alone cannot do anything for a company except to
illustrate what is being expended in specific areas related to quality
and highlight opportunities for cost improvement.

TO put quaity costs to use, it's necessary to organize them in a manner
that will support analysis. Use them to raise questions such as these: -
Did you know that for every \$100 in shipments, we lose \$5 in internal
distribution and handling failure costs?

Questions such as these immediately show the value of quality costs in
direct relation to known costs expenditures.

To determine exactly where to establish short-range quality cost trend
charts and goals, it is necessary to review the company's basic quality
measurement system-

Real improvement depends on actions within the baisc quality measurement
and crrective action system, enhance by the use of quality costs as an
important suppot tool. Specific uses of quality costs, therefore, must
be correlated to specific quality measurement target areas for
improvement.

    \hypertarget{dmaic}{%
\section{DMAIC}\label{dmaic}}

\hypertarget{define}{%
\subsection{Define}\label{define}}

\hypertarget{measure}{%
\subsection{Measure}\label{measure}}

\hypertarget{analyze}{%
\subsection{Analyze}\label{analyze}}

\hypertarget{improve}{%
\subsection{Improve}\label{improve}}

\hypertarget{control}{%
\subsection{Control}\label{control}}


    % Add a bibliography block to the postdoc
    
    
    
\end{document}
